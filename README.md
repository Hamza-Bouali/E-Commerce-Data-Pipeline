
# ğŸ›’ E-Commerce Data Pipeline (No Kafka)

A near-real-time data engineering pipeline that simulates an e-commerce platform. It ingests, transforms, stores, and visualizes customer and sales data using modern data stack tools â€” **without using Kafka**.

---

## ğŸ“Š Project Overview

This project demonstrates an end-to-end data pipeline:

- ğŸ”„ Simulates raw data (CSV) for orders, products, and users
- ğŸ“¥ Ingests data into a database (PostgreSQL)
- ğŸ”§ Transforms data using PySpark or dbt
- ğŸ—ƒ Stores processed data in a warehouse (PostgreSQL / Parquet)
- ğŸ“ˆ Visualizes metrics using Apache Superset

---

## ğŸ§± Architecture

```

Simulated CSV Data
â†“
Python Ingestion Scripts / Airflow DAGs
â†“
Transformations (Spark or dbt)
â†“
Storage (PostgreSQL + Parquet)
â†“
Analytics Dashboard (Superset)

```

---

## ğŸ› ï¸ Tech Stack

| Layer            | Tool                  |
|------------------|------------------------|
| Data Simulation   | Python                |
| Ingestion         | Python Scripts / Airflow |
| Processing        | PySpark or dbt        |
| Storage           | PostgreSQL, Parquet   |
| Orchestration     | Apache Airflow        |
| Visualization     | Apache Superset       |

---

## ğŸ“ Project Structure

```

ecommerce-data-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate\_data.py
â”‚   â””â”€â”€ ingest\_to\_db.py
â”œâ”€â”€ spark\_jobs/
â”‚   â””â”€â”€ transform\_orders.py
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl\_pipeline.py
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ models/
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ screenshots/
â””â”€â”€ README.md

````

---

## ğŸš€ Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/ecommerce-data-pipeline.git
cd ecommerce-data-pipeline
````

### 2. Set Up the Environment

* Install Python 3.8+
* Install PostgreSQL (or use Docker)
* Install dependencies:

```bash
pip install -r requirements.txt
```

### 3. Run Data Simulation

```bash
python scripts/generate_data.py
```

This creates CSVs in `data/raw/`.

### 4. Ingest to PostgreSQL

```bash
python scripts/ingest_to_db.py
```

### 5. Run Transformations

**Using Spark:**

```bash
spark-submit spark_jobs/transform_orders.py
```

**Or using dbt:**

```bash
cd dbt
dbt run
```

### 6. Launch Dashboard

* Start Superset and connect to your PostgreSQL
* Build charts for:

  * Daily Revenue
  * Orders by Category
  * Top Customers

---

## ğŸ“· Screenshots

Coming soon...

---

## ğŸ“Œ Key Learnings

* Data ingestion with Python and PostgreSQL
* Transformation workflows with Spark/dbt
* Workflow automation using Airflow
* Real-time dashboarding with Superset

---

## ğŸ§  Future Enhancements

* Add Great Expectations for data quality checks
* Move storage to cloud (e.g., S3 + BigQuery)
* Automate end-to-end pipeline with Airflow

---

## ğŸ‘¨â€ğŸ’» Author

* [Hamza Bouali]([https://github.com/yourusername](https://github.com/Hamza-Bouali))
